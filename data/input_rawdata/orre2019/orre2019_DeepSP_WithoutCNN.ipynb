{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score,precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def makemap(df0, r, e=1e-6):\n",
    "    df = np.array(deepcopy(df0))\n",
    "    n, m = df.shape\n",
    "    f = int(m / r)\n",
    "    df = np.hstack([df[:, j * f:(j + 1) * f] /\n",
    "                    df[:, j * f:(j + 1) * f].sum(1).reshape(-1, 1) for j in range(r)])\n",
    "    f_mean = df.mean(1)\n",
    "    data = np.zeros((n, 1, m, m), dtype=np.float)\n",
    "    for x in range(n):\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                if i < j:\n",
    "                    data[x, 0, i, j] = df[x, i] - df[x, j]\n",
    "                if i == j:\n",
    "                    data[x, 0, i, j] = df[x, i] - f_mean[x]\n",
    "                if i > j:\n",
    "                    data[x, 0, i, j] = np.tanh(np.log2((df[x, i] + e) / (df[x, j] + e)))\n",
    "    return data.round(3)\n",
    "\n",
    "\n",
    "class channel_attention(nn.Module):\n",
    "    def __init__(self, channel, ratio=2):\n",
    "        super(channel_attention, self).__init__()\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // ratio, False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel // ratio, channel, False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        max_pool_out = self.max_pool(x).view([b, c])\n",
    "        avg_pool_out = self.avg_pool(x).view([b, c])\n",
    "\n",
    "        max_fc_out = self.fc(max_pool_out)\n",
    "        avg_fc_out = self.fc(avg_pool_out)\n",
    "\n",
    "        out = max_fc_out + avg_fc_out\n",
    "        out = self.sigmoid(out).view([b, c, 1, 1])\n",
    "\n",
    "        return out * x\n",
    "\n",
    "\n",
    "class spacial_attention(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(spacial_attention, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, 1, padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_pool_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        pool_out = torch.cat([max_pool_out, avg_pool_out], dim=1)\n",
    "\n",
    "        out = self.conv(pool_out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # print(out)\n",
    "\n",
    "        return out * x\n",
    "\n",
    "\n",
    "class DeepSP_nocnn(nn.Module):\n",
    "    def __init__(self, fraction, out_channels, channel=32, ratio=2, kernel_size=3):\n",
    "        super(DeepSP_nocnn, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3, 1, 1),\n",
    "                                   nn.BatchNorm2d(32), nn.ReLU(),\n",
    "               #                   nn.MaxPool2d(kernel_size=2)\n",
    "                                  )\n",
    "\n",
    "        self.channel_attention = channel_attention(channel, ratio)\n",
    "        self.spacial_attention = spacial_attention(kernel_size)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(fraction ** 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3))\n",
    "\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(32 * (fraction // 2) ** 2, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.3))\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3))\n",
    "\n",
    "        self.fc3 = nn.Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    " #       x = self.conv3(x)\n",
    "#         x = self.channel_attention(x)\n",
    "#         x = self.spacial_attention(x)\n",
    "        x = self.fc1(x.view(x.size(0), -1))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MultiFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=0):\n",
    "        super(MultiFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        if self.alpha is None and self.gamma == 0:\n",
    "            focal_loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "        elif self.alpha is not None and self.gamma == 0:\n",
    "            self.alpha = torch.tensor(self.alpha) / torch.tensor(self.alpha).sum()\n",
    "            self.alpha = self.alpha.to(outputs)\n",
    "            focal_loss = F.cross_entropy(outputs, targets, weight=self.alpha)\n",
    "\n",
    "        elif self.alpha is None and self.gamma != 0:\n",
    "            ce_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
    "            p_t = torch.exp(-ce_loss)\n",
    "            focal_loss = ((1 - p_t) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "        else:\n",
    "            self.alpha = torch.tensor(self.alpha) / torch.tensor(self.alpha).sum()\n",
    "            self.alpha = self.alpha.to(outputs)\n",
    "            ce_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
    "            p_t = torch.exp(-ce_loss)\n",
    "            ce_loss = F.cross_entropy(outputs, targets, weight=self.alpha, reduction='none')\n",
    "            focal_loss = ((1 - p_t) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_db, index,  weight):\n",
    "    x_val, y_val = val_db\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "    loss_func = MultiFocalLoss(alpha=weight, gamma=2)\n",
    "    val_f1score_best = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            out = model(x_batch)\n",
    "            loss = loss_func(out, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(x_val)\n",
    "            loss = loss_func(val_out, y_val)\n",
    "            val_loss = loss.item()\n",
    "            y_valprob = sm(val_out)\n",
    "            y_valscore, y_valpred = torch.max(y_valprob, 1)\n",
    "            val_f1score = f1_score(y_val, y_valpred, average='macro')\n",
    "            val_acc = accuracy_score(y_val, y_valpred)\n",
    "\n",
    "#             if epoch % nepoch == 0:\n",
    "#                 print('[{}/{}] val_loss: {} | f1score: {} | accuracy: {}'\n",
    "#                       .format(epoch, epochs, val_loss, val_f1score, val_acc))\n",
    "\n",
    "            if val_f1score_best < val_f1score:\n",
    "                bestepoch = epoch\n",
    "                val_f1score_best = val_f1score\n",
    "                val_acc_best = val_acc\n",
    "                val_loss_best = val_loss\n",
    "                y_valprob_best = y_valprob\n",
    "                torch.save(model, 'model{}.pkl'.format(index + 1))\n",
    "\n",
    "#     print('bech epoch [{}/{}] | val_loss: {} | f1score: {} | accuracy: {}'\n",
    "#           .format(bestepoch, epochs, val_loss_best, val_f1score_best, val_acc_best))\n",
    "\n",
    "    return y_valprob_best.numpy()\n",
    "\n",
    "\n",
    "def cross_validation(x, y):\n",
    "    y_trainprob = np.zeros((y.shape[0], len(set(target))))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    cv_index = np.zeros(y.shape[0])\n",
    "    for index, (train_index, val_index) in enumerate(skf.split(x, y)):\n",
    "#         print('Cross validation: ', index + 1)\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        train_db = TensorDataset(x_train, y_train)\n",
    "        weight = 1 / np.bincount(y_train)\n",
    "        train_loader = DataLoader(train_db,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=0,\n",
    "                                  shuffle=True)\n",
    "        val_db = (x_val, y_val)\n",
    "        torch.manual_seed(seed)\n",
    "        model = DeepSP_nocnn(fraction, len(set(target)))\n",
    "        y_trainprob[val_index] = train(model, train_loader, val_db, index, weight)\n",
    "        cv_index[val_index] = index + 1\n",
    "\n",
    "    return y_trainprob, cv_index\n",
    "\n",
    "\n",
    "def predict(x_test):\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    y_testprob_result = np.zeros((x_test.shape[0], len(set(target))))\n",
    "    for index in range(n_splits):\n",
    "        model = torch.load('model{}.pkl'.format(index + 1))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_testprob_result += sm(model(x_test)).numpy() / n_splits\n",
    "    return y_testprob_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:48:35<00:00, 65.15s/it]\n"
     ]
    }
   ],
   "source": [
    "datafile = 'orre2019.csv'\n",
    "rep = 1\n",
    "\n",
    "n_splits = 5\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "l2 = 0.0\n",
    "data = pd.read_csv(datafile, index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "test_results = []\n",
    "test_f1scores = []\n",
    "test_accs = []\n",
    "test_precisions = []\n",
    "test_recalls = []\n",
    "test_size = 0.2\n",
    "\n",
    "data = data[data['markers'] != 'unknown']\n",
    "target = data.markers\n",
    "class_mapping = {label: idx for idx, label in enumerate(sorted(set(target)))}\n",
    "inv_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "train_results = []\n",
    "train_f1scores = []\n",
    "train_accs = []\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_quadlosses = []\n",
    "\n",
    "\n",
    "test_results = []\n",
    "test_f1scores = []\n",
    "test_accs = []\n",
    "test_precisions = []\n",
    "test_recalls = []\n",
    "test_quadlosses = []\n",
    "for seed in tqdm(range(100)):\n",
    "    test_set = pd.concat([data[data.markers == i].sample(math.ceil(data[data.markers == i].shape[0] * test_size))\n",
    "        for i in pd.unique(data.markers)],axis=0)\n",
    "    train_set = data[~data.index.isin(test_set.index)]\n",
    "    train_data = train_set.drop('markers', axis=1)\n",
    "    train_y = train_set.markers\n",
    "\n",
    "    test_data = test_set.drop('markers', axis=1)\n",
    "    test_y = test_set.markers\n",
    "  \n",
    "    x_train = torch.tensor(makemap(train_data.values, rep), dtype=torch.float)\n",
    "    y_train = torch.tensor(train_y.map(class_mapping).values)\n",
    "    fraction = x_train.shape[2]\n",
    "    y_trainprob, cv_index = cross_validation(x_train, y_train)\n",
    "    train_result = pd.DataFrame(data=y_trainprob, index=train_set.index, columns=list(class_mapping.keys()))\n",
    "    train_pred_array = train_result.values\n",
    "    \n",
    "    train_result['true_label'] = train_y\n",
    "    train_result['pred_label'] = y_trainprob.argmax(1)\n",
    "    train_result['pred_label'] = train_result['pred_label'].map(inv_class_mapping)\n",
    "    train_result['pred_prob'] = y_trainprob.max(1)\n",
    "    train_result['cv_index'] = cv_index\n",
    "    train_result['repeat'] = seed + 1\n",
    "    train_result = train_result.reset_index()\n",
    "    train_results.append(train_result)\n",
    "    train_f1scores.append(f1_score(train_result.true_label,\n",
    "                                  train_result.pred_label,average='macro'))\n",
    "    train_precisions.append(precision_score(train_result.true_label,\n",
    "                                           train_result.pred_label,average='macro'))\n",
    "    train_recalls.append(recall_score(train_result.true_label,\n",
    "                                     train_result.pred_label,average='macro'))\n",
    "    train_accs.append(accuracy_score(train_result.true_label, train_result.pred_label))\n",
    "    train_true_array = pd.get_dummies(train_result.true_label)[class_mapping.keys()].values\n",
    "    train_quadlosses.append(sum(sum(np.dot((train_true_array - train_pred_array),\n",
    "                                           (train_true_array - train_pred_array).T))) / train_result.shape[0])\n",
    "    \n",
    "    \n",
    "    x_test = torch.tensor(makemap(test_data.values, rep), dtype=torch.float)\n",
    "    y_testprob = predict(x_test)\n",
    "    \n",
    "    test_result = pd.DataFrame(data=y_testprob, index=test_set.index, columns=list(class_mapping.keys()))\n",
    "    test_pred_array = test_result.values\n",
    "    test_result['true_label'] = test_y\n",
    "    test_result['pred_label'] = y_testprob.argmax(1)\n",
    "    test_result['pred_label'] = test_result['pred_label'].map(inv_class_mapping)\n",
    "    test_result['pred_prob'] = y_testprob.max(1)\n",
    "    test_result['repeat'] = seed + 1\n",
    "    test_result = test_result.reset_index()\n",
    "    test_results.append(test_result)\n",
    "    \n",
    "    test_f1scores.append(f1_score(test_result.true_label,\n",
    "                                  test_result.pred_label,average='macro'))\n",
    "    test_precisions.append(precision_score(test_result.true_label,\n",
    "                                           test_result.pred_label,average='macro'))\n",
    "    test_recalls.append(recall_score(test_result.true_label,\n",
    "                                     test_result.pred_label,average='macro'))\n",
    "    test_accs.append(accuracy_score(test_result.true_label, test_result.pred_label))\n",
    "    test_true_array = pd.get_dummies(test_result.true_label)[class_mapping.keys()].values\n",
    "    test_quadlosses.append(sum(sum(np.dot((test_true_array - test_pred_array),\n",
    "                                           (test_true_array - test_pred_array).T))) / test_result.shape[0])\n",
    "\n",
    "train_results = pd.concat(train_results)\n",
    "test_results = pd.concat(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = datafile.split('.')[0]\n",
    "train_results.to_csv(f'{name}_DeepSP_nocnn_train_results.csv')\n",
    "test_results.to_csv(f'{name}_DeepSP_nocnn_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1score      0.759583\n",
       "precision    0.757470\n",
       "recall       0.766493\n",
       "accuracy     0.765910\n",
       "quadloss     4.308817\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainresult = pd.DataFrame({\n",
    "    'data': datafile.split('.')[0],\n",
    "    'method': 'WithoutCNN',\n",
    "    'f1score': train_f1scores,\n",
    "    'precision': train_precisions,\n",
    "    'recall': train_recalls,\n",
    "    'accuracy': train_accs,\n",
    "    'quadloss': train_quadlosses\n",
    "})\n",
    "trainresult.to_csv(name+'DeepSP_nocnn_train.csv', index=None)\n",
    "trainresult.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1score      0.751655\n",
       "precision    0.754129\n",
       "recall       0.757444\n",
       "accuracy     0.759720\n",
       "quadloss     1.216974\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testresult = pd.DataFrame({\n",
    "    'data': datafile.split('.')[0],\n",
    "    'method': 'WithoutCNN',\n",
    "    'f1score': test_f1scores,\n",
    "    'precision': test_precisions,\n",
    "    'recall': test_recalls,\n",
    "    'accuracy': test_accs,\n",
    "    'quadloss': test_quadlosses\n",
    "})\n",
    "testresult.to_csv(name+'DeepSP_nocnn_test.csv', index=None)\n",
    "testresult.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
